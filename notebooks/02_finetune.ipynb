{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2dadc0b-1734-44c0-a341-2cfd2a8b1367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 27 15:49:53 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  | 00000000:BA:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              71W / 700W |      4MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e4528f2-e1e4-43ff-8b09-9ac4ef837d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers datasets accelerate peft huggingface_hub hf_transfer flash-attn trl wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "206fd41e-11a0-436e-8d7e-241636dd3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"HF_TOKEN\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"WANDB_API_KEY\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"DeepLearningFinal\"\n",
    "os.environ[\"WANDB_NAME\"] = \"FineTuning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80562683-c01a-4daa-94f5-7eefaf1515ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from IPython.display import Markdown\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, EarlyStoppingCallback, TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c2fa64e-59e1-4fcf-b491-38b535e9ebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bfloat16 disponible: True\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Bfloat16 disponible: {torch.cuda.is_bf16_supported()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3eb014-c591-48b9-bcc4-bcc5140dfca8",
   "metadata": {},
   "source": [
    "### 1. Modelo y tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5093d395-b391-4abb-899e-261b49498077",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f8d5ad-779d-4671-98e3-686f16d0024a",
   "metadata": {},
   "source": [
    "#### 1.1 Cargar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "812f7d14-5c25-4db8-9939-b5b002ee304e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f10e2011ddc422b931a78b1686bab70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c57be3f8ab1475282ed04a6f1cf0c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf3023b9fd14ee6a1bd0ac77d6cc139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80202c58c274ca681d9861c4220d331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2922badae040d8af68c019bd70d7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f67c5ee7ab041acbb498e3778481a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5a08f250924987a39f530c642381fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c403585761c4ea694db5ed0b4a1701d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2aae11edf94d358864c9920e87c399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f5050e-7c66-48ef-8615-47bc5b742e7d",
   "metadata": {},
   "source": [
    "#### 1.2 Cargar tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "475741ef-2b69-42d1-8f86-2b48f9205aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6011d5bc72cc448586e787cf55c5bc3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641f15c5bb6a4500b54c872c3576337d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfed5a7ab9c941a6bd517dae0fbf6398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72eea8e8-0143-4b57-80e5-819d5e84fda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamano del vocabulario de meta-llama/Meta-Llama-3-8B-Instruct: 128,256\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tamano del vocabulario de {model_name}: {len(tokenizer.get_vocab()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a0258b3-da8a-4605-a220-f4d42d8c7988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f626d77b-d494-4979-8ef5-d1e8e5b296e6",
   "metadata": {},
   "source": [
    "- El modelo no tiene un token para padding, por ende se usa el token `end_of_sentence`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a5dd9f8-04cf-4bb0-96af-56adc3ac52e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b17d2d0-d49d-4062-9ec0-c6c1f00fa6e2",
   "metadata": {},
   "source": [
    "#### 1.3 Test de inferencia del modelo original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f1e17f1-5069-41b1-a3b4-82e8f1f98050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuracion de inferencia\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "generation_config = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0 + 1e-5,\n",
    "    \"top_k\": 100,\n",
    "    \"top_p\":0.90,\n",
    "    \"eos_token_id\": terminators\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9a89238-b4b0-41b2-a5ab-50e8847ce58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(messages: list, tokenizer, model, device, generation_config):\n",
    "    formatted_chat = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False\n",
    "    )\n",
    "    input_ids = tokenizer(text=formatted_chat, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**input_ids, **generation_config)\n",
    "    response = tokenizer.decode(token_ids=outputs[0], skip_special_tokens=True).split(\"assistant\")[-1].strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b9a6b-fbb7-4e18-9c5e-79b7616bfc75",
   "metadata": {},
   "source": [
    "- _ARTÍCULO 9o. CONDUCTA PUNIBLE. Para que la conducta sea punible se requiere que sea típica, antijurídica y culpable. La causalidad por sí sola no basta para la imputación jurídica del resultado. Para que la conducta del inimputable sea punible se requiere que sea típica, antijurídica y se constate la inexistencia de causales de ausencia de responsabilidad_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e2225c3-abf6-4438-b2cd-beb49f713d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En el Código Penal Colombiano, el artículo 9 establece las disposiciones generales sobre la responsabilidad penal. A continuación, te presento el texto del artículo 9:\n",
      "\n",
      "\"Artículo 9. Responsabilidad penal.\n",
      "\n",
      "La responsabilidad penal se establece cuando se cumplan los siguientes requisitos:\n",
      "\n",
      "1. La acción u omisión debe ser punible según la ley;\n",
      "2. La acción u omisión debe ser imputable al agente;\n",
      "3. La acción u omisión debe ser antijurídica, es decir, contraria a la ley o a los usos sociales;\n",
      "4. La acción u omisión debe ser culpable, es decir, no debe ser producto de una fuerza mayor o caso fortuito.\n",
      "\n",
      "La responsabilidad penal se presume en el agente que comete la acción o omisión punible, salvo que demuestre que no era imputable o que no era antijurídica o que no era culpable.\n",
      "\n",
      "La responsabilidad penal se extingue por la muerte del agente, salvo que se haya cometido la acción o omisión en el momento de la muerte o en el curso de la enfermedad que la causó, en cuyo caso se considera extinguida desde el momento de la muerte.\"\n",
      "\n",
      "En resumen, este artículo establece los requisitos para que un agente sea considerado responsable penalmente por una acción o omisión, y también regula la extinción de la responsabilidad penal en caso de muerte del agente.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Tu eres un asistente especializado en la ley penal colombiana\"},\n",
    "    {\"role\": \"user\", \"content\": \"Que dice el articulo 9 del codigo penal?\"},\n",
    "]\n",
    "response = model_inference(messages=messages, tokenizer=tokenizer, model=model, device=device, generation_config=generation_config)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaa25db-9770-4e8e-9ba3-63dbdf1cdb55",
   "metadata": {},
   "source": [
    "- _ARTÍCULO 11. ANTIJURIDICIDAD. Para que una conducta típica sea punible se requiere que lesione o ponga efectivamente en peligro, sin justa causa, el bien jurídicamente tutelado por la ley penal._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fe4058f-052c-4079-a301-d2de4fd9ac36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Según el Código Penal Colombiano, el artículo 11 establece:\n",
      "\n",
      "\"Artículo 11. La acción penal prescribe por el transcurso del plazo de diez años, contados a partir del día en que se cometió el delito. El plazo de prescripción se interrumpe por la presentación de la querella o la denuncia, y vuelve a correr desde el día en que se reanuda la investigación o se reabre el proceso.\n",
      "\n",
      "La prescripción no se aplica a los delitos de lesa humanidad, genocidio, crímenes de guerra y crímenes contra la humanidad, ni a los delitos cometidos por funcionarios públicos en ejercicio de sus funciones, ni a los delitos cometidos en perjuicio de la seguridad del Estado o de la paz pública.\n",
      "\n",
      "La prescripción no se aplica tampoco a los delitos cometidos en perjuicio de la persona del Presidente o del Vicepresidente de la República, del Presidente del Senado o de la Cámara de Representantes, del Fiscal General de la Nación o del Procurador General de la Nación, ni a los delitos cometidos en perjuicio de la integridad física o moral de los miembros del Congreso Nacional o de los jueces.\n",
      "\n",
      "La prescripción se interrumpe por la detención del imputado, y vuelve a correr desde el día en que se le conceda la libertad provisional o se le absuelva.\n",
      "\n",
      "La prescripción se aplica a los delitos cometidos por personas jurídicas, contando el plazo a partir de la fecha en que se cometió el delito y se interrumpe por la presentación de la querella o la denuncia, y vuelve a correr desde el día en que se reanuda la investigación o se reabre el proceso.\"\n",
      "\n",
      "Espero que esta información sea útil. Si necesitas más ayuda o tienes alguna pregunta adicional, no dudes en preguntar.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Tu eres un asistente especializado en la ley penal colombiana\"},\n",
    "    {\"role\": \"user\", \"content\": \"Que dice el articulo 11 del codigo penal?\"},\n",
    "]\n",
    "response = model_inference(messages=messages, tokenizer=tokenizer, model=model, device=device, generation_config=generation_config)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9122f5-faca-439f-9cdd-cab4b1fda19e",
   "metadata": {},
   "source": [
    "- Vemos que las respuestas no son del todo correctas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d815b4-e15c-4e6e-bf99-3ac599e26c5c",
   "metadata": {},
   "source": [
    "### 2. Datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "312c5377-9f5a-4db8-a047-05f8009d4115",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"jjovalle99/codigo_penal\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e700087-36da-46cc-95a5-277ce8ed7a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'articulo': 'ARTÍCULO 7o.',\n",
       " 'text': ' IGUALDAD. La ley penal se aplicará a las personas sin tener en cuenta consideraciones diferentes a las establecidas en ella. El funcionario judicial tendrá especial consideración cuando se trate de valorar el injusto, la culpabilidad y las consecuencias jurídicas del delito, en relación con las personas que se encuentren en las situaciones descritas en el inciso final del artículo 13 de la Constitución Política.',\n",
       " 'tokens': 96}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc10b8ca-79c4-4a10-a6b5-c3089b3e188f",
   "metadata": {},
   "source": [
    "#### 2.2 Preparar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92efd995-9f9c-4f84-b88f-5b3e693bda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": \"Tu eres un asistente especializado en la ley penal colombiana\"},\n",
    "        {\"role\": \"user\", \"content\": f'Que dice el {example[\"articulo\"]} del codigo penal?'},\n",
    "        {\"role\": \"assistant\", \"content\": f'El {example[\"articulo\"]} menciona que: {example[\"text\"]}'}\n",
    "    ]\n",
    "    return {\"formatted_chat\": tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d6ab050-e24d-46a4-8c4a-250da4ee81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23126ad9-76d4-4fd9-bec6-9b2568ab2e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'articulo': 'ARTÍCULO 2o.',\n",
       " 'text': ' INTEGRACIÓN. Las normas y postulados que sobre derechos humanos se encuentren consignados en la Constitución Política, en los tratados y convenios internacionales ratificados por Colombia, harán parte integral de este código.',\n",
       " 'tokens': 51,\n",
       " 'formatted_chat': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nTu eres un asistente especializado en la ley penal colombiana<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nQue dice el ARTÍCULO 2o. del codigo penal?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nEl ARTÍCULO 2o. menciona que:  INTEGRACIÓN. Las normas y postulados que sobre derechos humanos se encuentren consignados en la Constitución Política, en los tratados y convenios internacionales ratificados por Colombia, harán parte integral de este código.<|eot_id|>'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28f506-5d33-46a2-bf2e-ab21ceb91633",
   "metadata": {},
   "source": [
    "#### 2.2 Dividir en entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2433136-c0dd-4df6-81eb-9cd0ab72bf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 440 Val: 111\n"
     ]
    }
   ],
   "source": [
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=1399, shuffle=True)\n",
    "train_data = train_test_split[\"train\"].shuffle()\n",
    "val_data = train_test_split[\"test\"].shuffle()\n",
    "print(f\"Train: {len(train_data)}\", f\"Val: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20314ef-9c05-4318-8ac1-2ce6ebe9946b",
   "metadata": {},
   "source": [
    "### 3. Parameter Efficient Fine-Tuning (PEFT) - LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08cad201-b7ef-4962-9233-4e4b38f833da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaFlashAttention2(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4780f9a-f5d1-4744-8184-d9c30c37773c",
   "metadata": {},
   "source": [
    "#### 3.1 Preparar matrices LoRA para FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acfeb70d-94b7-4a24-b4bb-0513fd2bd570",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "if model.config.to_dict()[\"use_cache\"]:\n",
    "    model.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1b4f57a-0a36-4a47-ab8c-f98fc7bf47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "528e0c58-9400-4c0b-a14c-8cd165f2beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model=model, peft_config=peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b460886-b199-43da-852a-a66e9645b2fa",
   "metadata": {},
   "source": [
    "#### 3.2 Revisar parametros entrenables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f781d918-3d0d-4e35-bb09-11905bed24b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac200051-44ce-4e9e-83bf-16659a1218e0",
   "metadata": {},
   "source": [
    "### 4. Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b18a680-0913-4759-9170-5d86f770faea",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_definition = dict(\n",
    "    output_dir=\"/llama3-lora-codigopenal-dir\",\n",
    "    overwrite_output_dir=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    max_steps=1000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=20,\n",
    "    save_steps=20,\n",
    "    logging_first_step=True,\n",
    "    seed=1399,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    "    optim=\"adamw_torch_fused\"\n",
    ")\n",
    "args = TrainingArguments(**args_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd367cfb-95c3-4b8b-9878-258487773766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5393a74abe0f4e8bbc85db7fbf9100f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b4908a05114fe7af4a577fd7b588a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:342: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"formatted_chat\",\n",
    "    max_seq_length=2048,\n",
    "    packing=True,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb5faef8-b11a-4131-a97f-86b901458b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjj-ovalle\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240527_155915-dcgl79pl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jj-ovalle/DeepLearningFinal/runs/dcgl79pl' target=\"_blank\">/llama3-lora-codigopenal-dir</a></strong> to <a href='https://wandb.ai/jj-ovalle/DeepLearningFinal' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jj-ovalle/DeepLearningFinal' target=\"_blank\">https://wandb.ai/jj-ovalle/DeepLearningFinal</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jj-ovalle/DeepLearningFinal/runs/dcgl79pl' target=\"_blank\">https://wandb.ai/jj-ovalle/DeepLearningFinal/runs/dcgl79pl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 240/1000 35:38 < 1:53:47, 0.11 it/s, Epoch 43/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.412700</td>\n",
       "      <td>1.402117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.342800</td>\n",
       "      <td>1.277703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.182200</td>\n",
       "      <td>1.105194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.998300</td>\n",
       "      <td>0.944019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.798710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.708100</td>\n",
       "      <td>0.739035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.652700</td>\n",
       "      <td>0.707758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.604600</td>\n",
       "      <td>0.685477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.566000</td>\n",
       "      <td>0.669921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.526800</td>\n",
       "      <td>0.661031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.489100</td>\n",
       "      <td>0.656814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.451900</td>\n",
       "      <td>0.662918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=240, training_loss=0.81320614417394, metrics={'train_runtime': 2149.6827, 'train_samples_per_second': 14.886, 'train_steps_per_second': 0.465, 'total_flos': 7.122117922180301e+17, 'train_loss': 0.81320614417394, 'epoch': 43.63636363636363})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c57f4c-532d-4828-a1aa-3755afe3e5c0",
   "metadata": {},
   "source": [
    "#### 4.1 Comparar outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67710bda-a02e-4197-b934-6adca3554812",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3653dfd-6d6b-4d69-8dc3-47e7e7af9339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El ARTÍCULO 9. DE LA LEGALIDAD. declarado EXEQUIBLE por la Corte Constitucional mediante Sentencia C-148 de 2 de febrero de 1999, Magistrado Ponente Dr. Vladimiro Naranjo Trujillo, salvo el inciso 2o. en relación con el artículo 33 de la Ley 599 de 2000, sobre quien es el Juez de Ejecución de Penas y Medidas de Seguridad, y en el entendido de que la expresión 'la pena privativa de la libertad' comprende también las penas de prisión perpetua y prisión perpetua multiplica, y en el entendido de que la expresión 'la pena privativa de la libertad' comprende también las penas de prisión perpetua y prisión perpetua multiplica. \n",
      "Jurisprudencia Vigencia\n",
      "Corte Constitucional- La Corte Constitucional se declaró INHIBIDA de fallar sobre este artículo por ineptitud de la demanda, mediante Sentencia C-478-01 de 13 de septiembre de 2001, Magistrado Ponente Dr. Jaime Araújo Rentería. \n",
      "Notas de Vigencia\n",
      "- Ver la ADVERTENCIA y el Resumen de Notas de Vigencia al comienzo de este Código.Artículo modificado por el artículo 14 de la Ley 890 de 2004, publicada en el Diario Oficial No. 45.602, de 7 de julio de 2004, el cual establece en su versión original:'ARTÍCULO 14. Las penas previstas en los tipos penales contenidos en la Parte Especial del Código Penal se aumentarán en la tercera parte en el mínimo y en la mitad en el máximo. En todo caso, la aplicación de esta regla general de incremento deberá respetar el tope máximo de la pena privativa de la libertad para los tipos penales de acuerdo con lo establecido en el artículo 2o. de la presente ley....'El artículo 15, dispone: '... La presente ley rige a partir del 1o. de enero de 2005...' - Inciso 2o. adicionado por el artículo 36 de la Ley 1453 de 2011, public\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Tu eres un asistente especializado en la ley penal colombiana\"},\n",
    "    {\"role\": \"user\", \"content\": \"Que dice el articulo 9 del codigo penal?\"},\n",
    "]\n",
    "response = model_inference(messages=messages, tokenizer=tokenizer, model=fine_tuned_model, device=device, generation_config=generation_config)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62f230dc-2422-47bc-92d4-197a32ecd242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El ARTÍCULO 11. menciona que:  CIRCUNSTANCIAS DE ATENUACIÓN PUNITIVA. <Artículo modificado por el artículo 3 de la Ley 1453 de 2011. El nuevo texto es el siguiente:> Las penas privativas de derechos y las medidas de seguridad se atenuarán en los siguientes casos: \n",
      "1. Cuando el autor de la conducta sea menor de dieciocho (18) años. \n",
      "2. Cuando la conducta sea cometida por persona mayor de sesenta (60) años. \n",
      "3. Cuando la conducta sea cometida por persona mayor de cincuenta (50) años y menor de sesenta (60) años, y se trate de delitos cuya pena sea de prisión perpetua, prisión de noventa (90) a ciento ochenta (180) meses, prisión de sesenta (60) a ciento cuarenta y cuatro (144) meses, prisión de cuarenta y ocho (48) a noventa (90) meses, prisión de treinta y dos (32) a setenta y dos (72) meses, prisión de veinticuatro (24) a sesenta (60) meses, prisión de dieciocho (18) a cuarenta y ocho (48) meses, prisión de doce (12) a treinta y dos (32) meses, prisión de seis (6) a veinticuatro (24) meses, prisión de tres (3) a dieciocho (18) meses, prisión de uno (1) a doce (12) meses, prisión de uno (1) a seis (6) meses, prisión de uno (1) mes a tres (3) meses, prisión de uno (1) mes a dos (2) meses, prisión de uno (1) mes a uno (1) mes. \n",
      "4. Cuando la conducta sea cometida por persona mayor de cincuenta (50) años y menor de sesenta (60) años, y se trate de delitos cuya pena sea de prisión de cuarenta y ocho (48) a ciento ochenta (180) meses, prisión de treinta y dos (32) a ciento cuarenta y cuatro (144) meses, prisión de veinticuatro (24)\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Tu eres un asistente especializado en la ley penal colombiana\"},\n",
    "    {\"role\": \"user\", \"content\": \"Que dice el articulo 11 del codigo penal?\"},\n",
    "]\n",
    "response = model_inference(messages=messages, tokenizer=tokenizer, model=fine_tuned_model, device=device, generation_config=generation_config)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dfb822-0c05-4776-9a95-a6be57459dba",
   "metadata": {},
   "source": [
    "### 5. Guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1687eb17-02f9-455b-ad79-6c11c6c69ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = \"llama3-codigo-penal-colombiano\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b959807d-2a42-4f44-b0a5-7b7c2ba58384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1656d88a53fd4798bfd07e006271305e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94cdf1b55b804bfd8b03837e1595f2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b7b24ca7384351aae892377db97783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcb3d308bc842c7afb38005567e2a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5948516527946648edef000ec70283f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3a214f4ae0465198f8067608385f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jjovalle99/llama3-codigo-penal-colombiano/commit/9aad82e3fb181719db38ee7255def48b1206193f', commit_message='Upload tokenizer', commit_description='', oid='9aad82e3fb181719db38ee7255def48b1206193f', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guardar modelo y tokenizador\n",
    "fine_tuned_model.push_to_hub(model_save_name)\n",
    "tokenizer.push_to_hub(model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66bc3d49-16d0-4832-a23c-b70b1356f614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaba0143b34548de876f21f633730cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62a7912149144309429c2f9f64ebc36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f6bc5571d2444893e5b71ba919e5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jjovalle99/llama3-lora-codigopenal-dir/commit/3782af823f1bffa43215eeac562bf63a1a5b2390', commit_message='llama3-codigo-penal-colombianoadapters', commit_description='', oid='3782af823f1bffa43215eeac562bf63a1a5b2390', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guardar adapters\n",
    "trainer.push_to_hub(model_save_name + \"adapters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3096499a",
   "metadata": {},
   "source": [
    "![img](../assets/modelo.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
